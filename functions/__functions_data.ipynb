{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c37138-639f-40da-bd4c-d6895ae902cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth data\n",
    "\n",
    "def smoothed(x, n=300, k=3, y=None, return_type='df', datetime_index=False):\n",
    "    '''\n",
    "    Smooth data for plots\n",
    "    \n",
    "    Arguments:\n",
    "    x: pd.DataFrame, pd.Series\n",
    "    y: array-type\n",
    "    n: length of linespace\n",
    "    k: smoothing scale\n",
    "    return_type: \n",
    "        - if 'array' - return x_new, y_new\n",
    "        - if 'dict' - returns dict with {'x': x_new, 'y': y_new}\n",
    "\n",
    "    If x == pd.DataFrame functon returns pd.DataFrame anyway\n",
    "\n",
    "    Libraries:\n",
    "    from scipy.interpolate import make_interp_spline, BSpline\n",
    "    '''\n",
    "\n",
    "    if datetime_index:\n",
    "        start = x.index[0]\n",
    "        end = x.index[-1]\n",
    "        time_range = \\\n",
    "            pd.date_range(start=start, end=end, periods=n)\n",
    "        x = x.reset_index(drop=True)\n",
    "\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        var_name = x.columns[0] if x.columns[0] != 0 else 'variable'\n",
    "        x_index = x.index\n",
    "        x_new = np.linspace(x_index.min(), x_index.max(), n)\n",
    "        df = pd.DataFrame(index=x_new, columns=x.columns)\n",
    "        for col in x.columns:\n",
    "            y = x[col]\n",
    "            spl = scipy.interpolate.make_interp_spline(x_index, y, k=k)  # type: BSpline\n",
    "            y_new = spl(x_new)\n",
    "            df[col] = y_new\n",
    "        if return_type == 'df':\n",
    "            if datetime_index:\n",
    "                df.index = time_range\n",
    "            return df\n",
    "        if return_type == 'array':\n",
    "            return np.array(df.index), np.array(df.iloc[:, 0])\n",
    "        \n",
    "    elif isinstance(x, pd.Series):\n",
    "        var_name = x.name\n",
    "        y = x.copy()\n",
    "        x = x.index\n",
    "        \n",
    "        # n represents number of points to make between T.min and T.max\n",
    "        x_new = np.linspace(x.min(), x.max(), n) \n",
    "    \n",
    "        spl = scipy.interpolate.make_interp_spline(x, y, k=k)  # type: BSpline\n",
    "        y_new = spl(x_new)\n",
    "    \n",
    "        if return_type == 'dict':\n",
    "            if datetime_index:\n",
    "                ret_dict = {\n",
    "                    'x': time_range,\n",
    "                    'y': y_new\n",
    "                    }\n",
    "            else:\n",
    "                ret_dict = {\n",
    "                    'x': x_new,\n",
    "                    'y': y_new\n",
    "                    }\n",
    "            return ret_dict\n",
    "        elif return_type == 'array':\n",
    "            if datetime_index:\n",
    "                return time_range, y_new\n",
    "            else:\n",
    "                return x_new, y_new\n",
    "        elif return_type == 'df':\n",
    "            if datetime_index:\n",
    "                df = pd.DataFrame(data=y_new, index=time_range, columns=[var_name])\n",
    "            else:\n",
    "                df = pd.DataFrame(data=y_new, index=x_new, columns=[var_name])\n",
    "            return df\n",
    "    else:\n",
    "        y = x.copy()\n",
    "        x = arange(len(x))\n",
    "\n",
    "        # n represents number of points to make between T.min and T.max\n",
    "        x_new = np.linspace(x.min(), x.max(), n) \n",
    "    \n",
    "        spl = scipy.interpolate.make_interp_spline(x, y, k=k)  # type: BSpline\n",
    "        y_new = spl(x_new)\n",
    "        \n",
    "        if return_type == 'dict':\n",
    "            if datetime_index:\n",
    "                ret_dict = {\n",
    "                    'x': time_range,\n",
    "                    'y': y_new\n",
    "                    }\n",
    "            else:\n",
    "                ret_dict = {\n",
    "                    'x': x_new,\n",
    "                    'y': y_new\n",
    "                    }\n",
    "            return ret_dict\n",
    "        elif return_type == 'array':\n",
    "            if datetime_index:\n",
    "                return time_range, y_new\n",
    "            else:\n",
    "                return x_new, y_new\n",
    "        elif return_type == 'df':\n",
    "            if datetime_index:\n",
    "                df = pd.DataFrame(data=y_new, index=time_range, columns=['variable'])\n",
    "            else:\n",
    "                df = pd.DataFrame(data=y_new, index=x_new, columns=['variable'])\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f3677-3d20-453f-a524-efdee2836090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are NaNs in df\n",
    "\n",
    "def is_nan(df):\n",
    "    ret = df[df.isna().any(axis=1)]\n",
    "    shape = df[df.isna().any(axis=1)].shape\n",
    "    if shape[0] > 0:\n",
    "        return ret\n",
    "    else:\n",
    "        print(\"No NaN values in DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3573288-6dbd-4c0c-ae80-4f246e8b7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_describe(data):\n",
    "    \n",
    "    df = data.copy()\n",
    "    # varibles types\n",
    "    dtypes = df.dtypes.rename('Type').to_frame()\n",
    "    # frequency\n",
    "    frequency = df.count().rename('Count').to_frame()\n",
    "    # unique values\n",
    "    unique = df.nunique().rename('Unique').to_frame()\n",
    "    # NaNs\n",
    "    nans = df.isnull().sum().rename('NaN').to_frame()\n",
    "    # NaNs fraction\n",
    "    nans_frac = df.isnull().mean().round(2)\n",
    "    nans_frac = nans_frac.rename('Percentages').to_frame()\n",
    "    # list with results\n",
    "    results_list = [dtypes, frequency, unique, nans, nans_frac]\n",
    "    # df with results\n",
    "    results = pd.concat(results_list, axis=1)\n",
    "    results['Percentages'] = (results['Percentages'] * 100).astype('int64')\n",
    "    results = results.sort_values(['NaN'], ascending=False)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e69162-437b-4836-9867-3f70ceeb3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_bootstrap(\n",
    "        data, statistic=np.mean, n_bootstrap=1000,\n",
    "        confidence_level=0.95, random_state=42):\n",
    "    '''\n",
    "    Returns: dict(statistic, std, ci_min, ci_max, margin)\n",
    "    '''\n",
    "    data_ = (data,)\n",
    "    bootstrap = scipy.stats.bootstrap(\n",
    "        data=data_,\n",
    "        statistic=statistic,\n",
    "        n_resamples=n_bootstrap,\n",
    "        confidence_level=confidence_level,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    ci_min = bootstrap.confidence_interval[0]\n",
    "    ci_max = bootstrap.confidence_interval[1]\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        stat = data.apply(statistic)\n",
    "        stat = np.array(stat)\n",
    "        std = np.array(np.std(data, ddof=1))\n",
    "    else:\n",
    "        stat = statistic(data)\n",
    "        std = np.std(data, ddof=1)\n",
    "    margin = stat - ci_min\n",
    "\n",
    "    return_dct = {\n",
    "        'statistic': stat,\n",
    "        'std': std,\n",
    "        'ci_min': ci_min,\n",
    "        'ci_max': ci_max,\n",
    "        'margin': margin,\n",
    "    }\n",
    "    return return_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7c8a1-48ea-4fbb-b723-2386a7ad10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_t_distribution(\n",
    "        data=None, mean=None, std=None, n=None, confidence_level=0.95):\n",
    "\n",
    "    if data is not None:\n",
    "        arr = np.array(data)\n",
    "        n = len(arr)\n",
    "        mean = np.mean(arr)\n",
    "        se = scipy.stats.sem(arr)\n",
    "        \n",
    "    if mean and std and n is not None:\n",
    "        se = std / np.sqrt(n)\n",
    "\n",
    "    t = scipy.stats.t.ppf((1+confidence_level) / 2, n-1)\n",
    "    margin = t * se\n",
    "    ci_min = mean - margin\n",
    "    ci_max = mean + margin\n",
    "\n",
    "    return_dct = {\n",
    "        'mean': mean,\n",
    "        'ci_min': ci_min,\n",
    "        'ci_max': ci_max,\n",
    "        'margin': margin,\n",
    "        't': t\n",
    "    }\n",
    "    return return_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383bee6e-fc33-487d-be56-0294703199aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normality tests\n",
    "\n",
    "def test_normality(data, alpha=0.05):\n",
    "    \n",
    "    tests_names = []\n",
    "    pvalue = []\n",
    "    condition = []\n",
    "        \n",
    "    # Kolmogorov-Smirnov\n",
    "    ks = stats.kstest(data, 'norm')\n",
    "    pvalue_ks = ks.pvalue\n",
    "    tests_names.append('Kolmogorov-Smirnov')\n",
    "    pvalue.append(pvalue_ks)\n",
    "    if pvalue_ks < alpha:\n",
    "        condition.append('Not normal')\n",
    "    else:\n",
    "        condition.append('Normal')\n",
    "\n",
    "    # Anderson-Darling\n",
    "    and_dar = stats.anderson(data, dist='norm')\n",
    "    and_dar_sign = and_dar.critical_values[2]\n",
    "    and_dar_statistic = and_dar.statistic\n",
    "    tests_names.append('Anderson-Darling (s)')\n",
    "    pvalue.append(and_dar_statistic)\n",
    "    if and_dar_statistic > and_dar_sign:\n",
    "        condition.append('Not normal')\n",
    "    else:\n",
    "        condition.append('Normal')\n",
    "\n",
    "    # Shapiro-Wilk\n",
    "    pvalue_sw = stats.shapiro(data).pvalue\n",
    "    tests_names.append('Shapiro-Wilk')\n",
    "    pvalue.append(pvalue_sw)\n",
    "    if pvalue_sw < alpha:\n",
    "        condition.append('Not normal')\n",
    "    else:\n",
    "        condition.append('Normal')\n",
    "\n",
    "    # jarque-bera test\n",
    "    jb_name = [\"Jarque-Bera\", \"Chi^2\", \"Skew\", \"Kurtosis\"]\n",
    "    jb_statistic = sms.jarque_bera(data)\n",
    "    jb = dict(zip(jb_name, jb_statistic))\n",
    "    pvalue_jb = jb['Chi^2']\n",
    "    tests_names.append('Jarque-Bera')\n",
    "    pvalue.append(pvalue_jb)\n",
    "    if pvalue_jb < alpha:\n",
    "        condition.append('Not normal')\n",
    "    else:\n",
    "        condition.append('Normal')\n",
    "    \n",
    "    # D’Agostino and Pearson\n",
    "    dagp = stats.normaltest(data)\n",
    "    pvalue_dagp = dagp.pvalue\n",
    "    tests_names.append('D’Agostino-Pearson')\n",
    "    pvalue.append(pvalue_dagp)\n",
    "    if pvalue_dagp < alpha:\n",
    "        condition.append('Not normal')\n",
    "    else:\n",
    "        condition.append('Normal')\n",
    "\n",
    "    pvalue = [np.round(i, 4) for i in pvalue]\n",
    "    results_df = pd.DataFrame({\n",
    "        'Test': tests_names,\n",
    "        'P or Statistic (s)': pvalue,\n",
    "        'Condition': condition,\n",
    "    })\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e9896-28f6-457d-9dbd-a0b1d6a449c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_display(\n",
    "        features, importance,\n",
    "        top=None, imp_min_level=None, only_features=True):\n",
    "\n",
    "    '''\n",
    "     \n",
    "    '''\n",
    "\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importance\n",
    "    })\n",
    "    if imp_min_level is not None:\n",
    "        loc_row = feature_importance['Importance'] > imp_min_level\n",
    "        feature_importance = (feature_importance\n",
    "                              .loc[loc_row, :]\n",
    "                              .sort_values('Importance', ascending=False)\n",
    "                              .reset_index(drop=True))\n",
    "    if top is not None:\n",
    "        feature_importance = (feature_importance\n",
    "                             .sort_values('Importance', ascending=False)\n",
    "                             .reset_index(drop=True))\n",
    "        feature_importance = feature_importance.loc[0:top-1]\n",
    "\n",
    "    if only_features:\n",
    "        feature_importance = feature_importance['Feature']\n",
    "        \n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d32e5c-c3b1-46a6-836b-26a86ef1bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_column_iqr(data, feature, scale=1.5):\n",
    "\n",
    "    '''\n",
    "    Add nominative (1/0) column '{feature}_is_out' in DataFrame, that indicates outliers for Feature\n",
    "    '''\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    q1 = df[feature].quantile(0.25)\n",
    "    q3 = df[feature].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_boundary = q1 - scale*iqr\n",
    "    upper_boundary = q3 + scale*iqr\n",
    "    condition = ((df[feature] < lower_boundary) |\n",
    "                 (df[feature] > upper_boundary))\n",
    "    df[feature+'_is_out'] = condition.astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f4da8-115d-4638-ab0b-881d1a351a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_w_target(data, target):\n",
    "\n",
    "    '''\n",
    "    Create sorted DataFrame with correlations to Target \n",
    "    '''\n",
    "    \n",
    "    df = (data\n",
    "          .corr()[target]\n",
    "          .sort_values(ascending=False, key=abs)[1:]\n",
    "          .to_frame())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ab501-a85a-45ee-b547-7b52f000f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns_match(data):\n",
    "\n",
    "    '''\n",
    "    Check if all columns in DataFrame are equal and return no equal if not\n",
    "    '''\n",
    "\n",
    "    df = data.copy()\n",
    "    df['is_equal'] = df.eq(df.iloc[:, 0], axis=0).all(1).astype(int)\n",
    "    equal_sum = df['is_equal'].sum()\n",
    "\n",
    "    if equal_sum == len(df):\n",
    "        print('All values matched')\n",
    "        return None\n",
    "    else:\n",
    "        loc = df['is_equal'] == 0, df.columns != 'is_equal'\n",
    "        result = df.loc[loc].copy()\n",
    "        return result      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2bbff0-051e-4e1a-bdc3-fbbf82dba72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_na(data, features_list):\n",
    "\n",
    "    '''\n",
    "    Fill all NaNs in DataFrame by 'NA'\n",
    "    '''\n",
    "\n",
    "    df = data.copy()\n",
    "    for feature in features_list:\n",
    "        df[feature] = df[feature].fillna('NA')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d7a78-4e04-4843-a9c7-bd08e8391459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_by_first(data, return_type='df'):\n",
    "\n",
    "    '''\n",
    "    Normalize kind: \n",
    "        first_value == first_value\n",
    "        second_value = second_value / first_value\n",
    "        third_value = third_value / first_value\n",
    "    '''\n",
    "    \n",
    "    first_value = data[0]\n",
    "    \n",
    "    data_new = [(x/first_value) for x in data]\n",
    "\n",
    "    if return_type == 'df':\n",
    "        df = pd.DataFrame(data=data_new, index=data.index)\n",
    "        return df\n",
    "    if return_type == 'series':\n",
    "        series = pd.Series(data=data_new, index=data.index)\n",
    "        return series\n",
    "    elif return_type == 'array':\n",
    "        array = np.array(data_new)\n",
    "        return array\n",
    "    elif return_type == 'list':\n",
    "        lst = list(data_new)\n",
    "        return lst\n",
    "    else:\n",
    "        print(\"'return_type' must be 'df', 'series', 'array', 'list'\")\n",
    "    \n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2a7af-fcf4-41a0-ac7d-fbaf04bd47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(data, reshape=True, return_type='df'):\n",
    "\n",
    "    '''\n",
    "    MinMaxScaler 0/1 \n",
    "    '''\n",
    "    \n",
    "    if (isinstance(data, pd.Series) | \n",
    "        isinstance(data, pd.DataFrame)):\n",
    "        idxs = data.index.copy()\n",
    "    if reshape:\n",
    "        data = np.array(data).reshape(-1, 1)\n",
    "    data_new = MinMaxScaler().fit_transform(data)\n",
    "    if return_type == 'df':\n",
    "        data_new = pd.DataFrame(data=data_new, index=idxs)\n",
    "    elif return_type == 'array':\n",
    "        pass\n",
    "    else:\n",
    "        print(\"return_type must be 'df' or 'array'\")\n",
    "        return None\n",
    "        \n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ecd55-8dcf-4974-bf26-47a44816aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness(df):\n",
    "\n",
    "    df = pd.DataFrame(df.skew(numeric_only=True),\n",
    "                      columns=['Skewness'],\n",
    "                      index=None)\n",
    "\n",
    "    df['Highly skewed'] = (abs(df['Skewness']) > 0.5)\n",
    "    df['abs'] = abs(df['Skewness'])\n",
    "\n",
    "    df = df.sort_values(by=['abs', 'Highly skewed'], ascending=False)\n",
    "    df = df.drop('abs', axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b841ad9-bea0-4e7d-9c4b-fbf38106f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kurtosis(df):\n",
    "\n",
    "    df = pd.DataFrame(df.kurtosis(numeric_only=True),\n",
    "                      columns=['Kurtosis'],\n",
    "                      index=None)\n",
    "    df['Type'] = np.nan\n",
    "\n",
    "    df.loc[df['Kurtosis'] > 1, 'Type'] = 'Too Peaked'\n",
    "    df.loc[df['Kurtosis'] < -1, 'Type'] = 'Too Flat'\n",
    "    df.loc[(df['Kurtosis'] <= 1) & (df['Kurtosis'] >= -1), 'Type'] = 'Normal'\n",
    "    \n",
    "    df['abs'] = abs(df['Kurtosis'])\n",
    "    df = df.sort_values(by=['abs', 'Type'], ascending=False)\n",
    "    df = df.drop('abs', axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc657ab-e995-4630-af72-342906c3bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acf(\n",
    "        acf_w_alphas=None, data=None, lags=40, partial=False, scatter=False, s=2,\n",
    "        transparency_lines=1, color_lines=None, exclude_first=True,\n",
    "        transparency_significant=0.15, show_last_significant=True,\n",
    "        last_significant_delta=0.1, color_significant=None, **kwargs):\n",
    "\n",
    "    if acf_w_alphas is None:\n",
    "        acf_w_alphas = ts_acf_calculate(data, lags=lags, partial=partial, **kwargs) \n",
    "        \n",
    "    acf = acf_w_alphas[:, 0]\n",
    "    alphas = acf_w_alphas[:, 1:]\n",
    "    \n",
    "    lags = len(acf)\n",
    "    xticks = arange(lags)\n",
    "    color_palette = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "    color_significant = color_significant or color_palette[2]\n",
    "    color_lines = color_lines or color_palette[0]\n",
    "\n",
    "    if exclude_first:\n",
    "        acf[0] = 0\n",
    "        alphas[:1] = 0\n",
    "\n",
    "    if scatter:\n",
    "        plt.scatter(\n",
    "            x=xticks,\n",
    "            y=acf,\n",
    "            s=s\n",
    "        )\n",
    "    for i in arange(lags):\n",
    "        plt.plot(\n",
    "            [i, i],\n",
    "            [0, acf[i]],\n",
    "            color=color_lines,\n",
    "            alpha=transparency_lines\n",
    "        )\n",
    "    if exclude_first:\n",
    "        plt.fill_between(\n",
    "            arange(lags)[1:],\n",
    "            (alphas[:, 0] - acf)[1:],\n",
    "            (alphas[:, 1] - acf)[1:],\n",
    "            lw=0,\n",
    "            color=color_significant,\n",
    "            alpha=transparency_significant\n",
    "        )\n",
    "    else:\n",
    "        plt.fill_between(\n",
    "            arange(lags),\n",
    "            alphas[:, 0] - acf,\n",
    "            alphas[:, 1] - acf,\n",
    "            lw=0,\n",
    "            color=color_significant,\n",
    "            alpha=transparency_significant\n",
    "        )\n",
    "\n",
    "    if show_last_significant:\n",
    "        last_sign = ts_acf_last_significant_index(data=data, partial=partial)\n",
    "        pacf_text = f'{last_sign}'\n",
    "        last_sign_y = acf[last_sign] + last_significant_delta\n",
    "    \n",
    "        plt.annotate(\n",
    "            text=pacf_text,\n",
    "            xy=(last_sign, last_sign_y),\n",
    "            ha='center',\n",
    "            size=9,\n",
    "            color=palette[1],\n",
    "            weight='bold')\n",
    "\n",
    "    plt.plot([-1, lags], [0, 0])\n",
    "    plt.gca().spines[['bottom', 'left']].set_visible(False)\n",
    "    plt.grid(False)\n",
    "    plt.xlim(-2, lags+1)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7be56e-4bcb-4aa4-a6d6-5fc455c77a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_acf_calculate(data, lags=36, alpha=0.05, partial=False, **kwargs):\n",
    "\n",
    "    if partial:\n",
    "        acf_result = statsmodels.tsa.stattools.pacf(\n",
    "            data, nlags=lags, alpha=alpha, method='ywadjusted', **kwargs)\n",
    "    else:\n",
    "        acf_result = statsmodels.tsa.stattools.acf(\n",
    "            data, nlags=lags, alpha=alpha, missing='none', **kwargs)\n",
    "\n",
    "    acf = acf_result[0]\n",
    "    alphas = acf_result[1]\n",
    "    result = np.hstack([acf.reshape(-1,1), alphas])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18464e0c-294a-4961-9049-1bc6c04c8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_acf_last_significant_index(data, lags=36, partial=False):\n",
    "    '''\n",
    "    Return index of first insignificant element in ACF or PACF\n",
    "\n",
    "    Attributes:\n",
    "        ci - confident intervals for ACF value (example, result[1] of statsmodels.tsa.stattools.acf)\n",
    "    '''\n",
    "    acf = ts_acf_calculate(data, lags=lags, partial=partial)\n",
    "    ci = acf[:, 1:]\n",
    "    \n",
    "    # for i, j in enumerate(ci):\n",
    "    #     status = np.all(j > 0) if j[0] > 0 else np.all(j < 0)\n",
    "    #     if not status:\n",
    "    #         break\n",
    "    for i, j in enumerate(ci):\n",
    "        # check if values in 'alphas' have not equal sign\n",
    "        if ((j[0]<0) != (j[1]<0)):\n",
    "            return i-1\n",
    "        elif i == len(ci)-1:\n",
    "            print(f'All {lags} lags significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41001246-16a2-453d-8383-e5bbf258466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_arima_forecast(model, steps, data, ci=[80, 95]):\n",
    "\n",
    "    df = data.copy()\n",
    "    results = model.get_forecast(steps=steps)\n",
    "\n",
    "    final_df = pd.DataFrame(\n",
    "        index = pd.date_range(\n",
    "            df.index[0], results.predicted_mean.index[-1], freq=df.index.freq),\n",
    "        data=pd.concat([\n",
    "            df.iloc[:,0], results.predicted_mean], axis=0),\n",
    "        columns=['data'])\n",
    "\n",
    "    \n",
    "    final_df['forecast'] = np.where(\n",
    "        final_df.index.date < results.predicted_mean.index[0].date(), 0, 1)\n",
    "\n",
    "    for ci_value in ci:\n",
    "        alpha = (100 - ci_value) / 100\n",
    "        final_df[f'lower_ci{ci_value}'] = \\\n",
    "            results.conf_int(alpha=alpha).iloc[:, 0]\n",
    "        final_df[f'upper_ci{ci_value}'] = \\\n",
    "            results.conf_int(alpha=alpha).iloc[:, 1]\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b2abc-90fc-46b7-8bbd-f1205fbe6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_poisson_bootstrap(\n",
    "        data1, \n",
    "        data2,\n",
    "        n_bootstrap=10000,\n",
    "        ci=[2.5,97.5],\n",
    "        rnd = 3,\n",
    "        plot=True,\n",
    "        figsize=(7, 2),\n",
    "        colors=None,\n",
    "        execution_time=True,\n",
    "        results_dict=False,\n",
    "        means_plots=True,\n",
    "        rstyle=True,\n",
    "        rstyle_dataplot_kwargs={},\n",
    "        rstyle_meansplot_kwargs={},\n",
    "        simple_results=False):\n",
    "\n",
    "    '''\n",
    "    If plot == True and results_dict == True and means_plots == True\n",
    "    Returns: \n",
    "        - dict with means difference value and boundaries\n",
    "        - dict with Poisson bootstrap folds\n",
    "        - figure with means diffrenece plot with boundaries\n",
    "        - figure with data plots\n",
    "    '''\n",
    "\n",
    "    if simple_results:\n",
    "        plot = False\n",
    "        execution_time = False\n",
    "        results_dict = False\n",
    "        means_plots = False\n",
    "    \n",
    "    t_start = time.time()\n",
    "\n",
    "    if colors is None:\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "    color0 = colors[0]\n",
    "    color1 = colors[1]\n",
    "    color2 = '#505050'\n",
    "\n",
    "    if not isinstance(data1, np.ndarray):\n",
    "        data1 = np.array(data1)\n",
    "    if not isinstance(data2, np.ndarray):\n",
    "        data2 = np.array(data2)\n",
    "    \n",
    "    mean1 = np.mean(data1)\n",
    "    mean2 = np.mean(data2)\n",
    "\n",
    "    means_diff = mean1 - mean2\n",
    "\n",
    "    poisson_bootstraps1 = stats.poisson(1).rvs(\n",
    "        (n_bootstrap, len(data1))).astype(np.int64)\n",
    "\n",
    "    poisson_bootstraps2 = stats.poisson(1).rvs(\n",
    "        (n_bootstrap, len(data2))).astype(np.int64)\n",
    "\n",
    "    mean1_boot = (poisson_bootstraps1*data1).sum(axis=1) / len(data1)\n",
    "    mean2_boot = (poisson_bootstraps2*data2).sum(axis=1) / len(data2)\n",
    "    means_diff_boot = mean1_boot - mean2_boot\n",
    "    \n",
    "    lower_boundary, upper_boundary = np.percentile(means_diff_boot, ci)\n",
    "    \n",
    "    means_dict = {\n",
    "        'mean1': mean1_boot,\n",
    "        'mean2': mean2_boot,\n",
    "        'means_diff': means_diff_boot\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'Lower Boundary': lower_boundary, \n",
    "        'Means Difference': means_diff, \n",
    "        'Upper Boundary': upper_boundary\n",
    "    }\n",
    "\n",
    "    if not simple_results:\n",
    "        print('\\n'+'         Poisson bootstrap summary')\n",
    "\n",
    "    if plot:\n",
    "\n",
    "        fig_data = plt.figure(figsize=figsize)\n",
    "\n",
    "        ax = sns.histplot(\n",
    "            means_diff_boot,\n",
    "            color=color0, alpha=0.5)\n",
    "\n",
    "        ylim = ax.get_ylim()[1]\n",
    "        \n",
    "        ax.vlines(\n",
    "            0, 0, ylim*0.1,\n",
    "            color=saturate_color(color1, 1.25), linewidth=2.5)\n",
    "        ax.vlines(\n",
    "            lower_boundary, 0, ylim*0.15,\n",
    "            color=color2, linewidth=1.5)\n",
    "        ax.vlines(\n",
    "            upper_boundary, 0, ylim*0.15,\n",
    "            color=color2, linewidth=1.5) \n",
    "\n",
    "        ax.set_ylabel('Count')\n",
    "\n",
    "        if rstyle:\n",
    "            axis_rstyle(**rstyle_dataplot_kwargs)\n",
    "\n",
    "        ax.legend(\n",
    "            **legend_inline(),\n",
    "            **legend_create_handles(\n",
    "                3, ['r', 'l', 'l'],\n",
    "                colors=[color0, color1, color2],\n",
    "                alphas=[0.5, 1, 1],\n",
    "                labels=['Means difference', 'Zero', 'Significance borders'],\n",
    "                linelength=1\n",
    "            ))\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    # the boundaries, measured by 1 and 99 percentiles,\n",
    "    # are equvivalent of p-value probabiblities boundaries an 0.05 significant level;\n",
    "    # if difference in means is out of boundaries range, we reject null hypotesis - \n",
    "    # it means that the difference if statistical significant\n",
    "    if lower_boundary < 0 < upper_boundary:\n",
    "        significancy = False\n",
    "    else: \n",
    "        significancy = True\n",
    "    \n",
    "    # check with Kolmogorov–Smirnov test if distribution of p-values is normal\n",
    "    # (previously standardize means differences with stats.zscore)\n",
    "    pvalue_ks = stats.kstest(stats.zscore(means_diff_boot), stats.norm.cdf).pvalue\n",
    "    \n",
    "    # Kolmogorov–Smirnov test null hypotesis: distribution of simulation pvalues is normal\n",
    "    # if pvalue due Kolmogorov–Smirnov test <= 0.05, \n",
    "    # we reject null hypotesis that distribution of pvalues due simulation is normal;  \n",
    "    if pvalue_ks <= 0.05:\n",
    "        distribution = 'not '\n",
    "    else:\n",
    "        distribution = ''\n",
    "\n",
    "    if not simple_results:\n",
    "\n",
    "        mean1_rnd = f\"%.{rnd}f\" % mean1\n",
    "        mean2_rnd = f\"%.{rnd}f\" % mean2\n",
    "        lower_boundary_rnd = f\"%.{rnd}f\" % lower_boundary\n",
    "        means_diff_rnd = f\"%.{rnd}f\" % means_diff\n",
    "        upper_boundary_rnd = f\"%.{rnd}f\" % upper_boundary\n",
    "        pvalue_rnd = f\"%.{rnd}f\" % pvalue_ks\n",
    "        \n",
    "        ha1 = '===================================================================================='\n",
    "        start1 = '          '\n",
    "        space = '              '\n",
    "\n",
    "        delta1 = 27 - len(f'Lower Boundary:{lower_boundary_rnd}')\n",
    "        delta1 = delta1 * ' '\n",
    "        delta2 = 27 - len(f'Means Difference:{means_diff_rnd}')\n",
    "        delta2 = delta2 * ' '\n",
    "        delta3 = 27 - len(f'Upper Boundary:{upper_boundary_rnd}')\n",
    "        delta3 = delta3 * ' '\n",
    "\n",
    "        delta4 = 43 - len(f'Significantly difference:{significancy}')\n",
    "        delta4 = delta4 * ' '\n",
    "        delta5 = 43 - len(f\"Means Differences' distribution:{distribution}normal\")\n",
    "        delta5 = delta5 * ' '\n",
    "        delta6 = 43 - len(f'Kolmogorov–Smirnov test p-value:{pvalue_rnd}')\n",
    "        delta6 = delta6 * ' '\n",
    "\n",
    "        print(\n",
    "            '\\n'\n",
    "            f'{start1}'f'Significantly difference:{delta4}\\033[1m{significancy}\\033[0m' \\\n",
    "                + space + f'Lower Boundary:{delta1}{lower_boundary_rnd}' '\\n' \n",
    "            f'{start1}'f\"Means Differences' distribution:{delta5}{distribution}normal\" \\\n",
    "                + space + f'Means Difference:{delta2}{means_diff_rnd}' '\\n' \n",
    "            f'{start1}'+f'Kolmogorov–Smirnov test p-value:{delta6}{pvalue_rnd}' \\\n",
    "                + space+ f'Upper Boundary:{delta3}{upper_boundary_rnd}' '\\n' '\\n' \n",
    "            f'{start1}' + ha1 + '\\n' '\\n' \\\n",
    "            f'{start1}' + f'Mean1: {mean1_rnd}' '\\n'\n",
    "            f'{start1}' + f'Mean2: {mean2_rnd}' '\\n'\n",
    "           )\n",
    "\n",
    "    if means_plots:\n",
    "        \n",
    "        fig_means = plt.figure(figsize=figsize)\n",
    "        \n",
    "        ax = sns.histplot(\n",
    "            mean1_boot,\n",
    "            color=color0, alpha=0.5)\n",
    "        \n",
    "        ax = sns.histplot(\n",
    "            mean2_boot, \n",
    "            color=color1, alpha=0.5)\n",
    "        \n",
    "        ax.set(xlabel=None)\n",
    "        ax.set_ylabel('Count', weight='bold')\n",
    "        \n",
    "        ylim = ax.get_ylim()[1]\n",
    "        \n",
    "        ax.vlines(\n",
    "            np.mean(mean1_boot), 0, ylim,\n",
    "            color=saturate_color(color0, 1.25), linewidth=1, ls='--')\n",
    "        ax.vlines(\n",
    "            np.mean(mean2_boot), 0, ylim,\n",
    "            color=saturate_color(color1, 1.25), linewidth=0.75, ls='--')\n",
    "\n",
    "        if rstyle:\n",
    "            axis_rstyle(**rstyle_meansplot_kwargs)\n",
    "\n",
    "        ax.legend(\n",
    "            **legend_inline(),\n",
    "            **legend_create_handles(\n",
    "                2, 's',\n",
    "                colors=[color0, color1],\n",
    "                alphas=[0.5, 0.5],\n",
    "                labels=['Data1 means', 'Data2 means']\n",
    "            ))\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    if execution_time:\n",
    "        \n",
    "        execution_time = np.round(time.time() - t_start, 2)\n",
    "        execution_time_formated = \\\n",
    "                         str(dt.timedelta(seconds=np.round(time.time() - t_start)))\n",
    "        \n",
    "        print('\\n'+f'{start1}'+'Execution time: {}'.format(execution_time_formated))\n",
    "        print(f'{start1}'+'Execution time (seconds): {}'.format(execution_time, '\\n'))\n",
    "\n",
    "    if results_dict:\n",
    "        return results, means_dict, fig_data, fig_means\n",
    "\n",
    "    if simple_results:\n",
    "        return significancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c99b2fb6-f54d-4742-8ad1-8398c8e9a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_diff_sunday(data, feature):\n",
    "\n",
    "    df = data.copy()\n",
    "    df['month'] = df.index.month\n",
    "    months_unique = df['month'].unique()\n",
    "    first_month = months_unique[0]\n",
    "    len_first_month = len(df[df['month']==first_month])\n",
    "    new_values = np.empty(len_first_month)\n",
    "    new_values[:] = np.NaN\n",
    "    new_index = df[df['month']==first_month].index\n",
    "    df['diff_Sunday'] = np.NaN\n",
    "\n",
    "    for month in months_unique:\n",
    "        if month > first_month:\n",
    "            index_current = df.loc[df['month']==month].index\n",
    "            index_previous = df.loc[df['month']==month-1].index\n",
    "            df_current = df.loc[index_current, :].copy()\n",
    "            df_previous = df.loc[index_previous, :].copy()\n",
    "            \n",
    "            df_s_previous = df_previous.loc[df_previous['weekday'] == 6].copy()\n",
    "            values_s_previous = df_s_previous.groupby(['hour', 'minute']).mean()[feature]\n",
    "            \n",
    "            df_nos_previous = df_previous.loc[df_previous['weekday'] != 6].copy()\n",
    "            values_nos_previous = df_nos_previous.groupby(['hour', 'minute']).mean()[feature]\n",
    "\n",
    "            values_diff = values_nos_previous - values_s_previous\n",
    "\n",
    "            current_s_number = int(len(df_current[df_current['weekday']==6]) / 144)\n",
    "            values_diff_current = list(values_diff) * current_s_number\n",
    "\n",
    "            df.loc[((df.index.isin(index_current)) & (df['weekday'] == 6)), 'diff_Sunday'] = values_diff_current\n",
    "\n",
    "            df['diff_Sunday'] = df['diff_Sunday'].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "65235b3d-d4c3-45fe-a6c9-59efe17e485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_previous_month(data, feature):\n",
    "\n",
    "    df = data.copy()\n",
    "    df['month'] = df.index.month\n",
    "    months_unique = df['month'].unique()\n",
    "    first_month = months_unique[0]\n",
    "    len_first_month = len(df[df['month']==first_month])\n",
    "    new_values = np.empty(len_first_month)\n",
    "    new_values[:] = np.NaN\n",
    "\n",
    "    for month in months_unique:\n",
    "        if month > first_month:\n",
    "            index_current = df.loc[df['month']==month].index\n",
    "            index_previous = df.loc[df['month']==month-1].index\n",
    "            len_current = len(df.loc[index_current])\n",
    "            len_previous = len(df.loc[index_previous])\n",
    "            values_previous = df.loc[index_previous, feature].copy()\n",
    "            if len_previous > len_current:\n",
    "                values_current = values_previous.iloc[:len_current].values.copy()\n",
    "            else:\n",
    "                values_current = values_previous.values.copy()\n",
    "                len_diff = len_current - len_previous\n",
    "                values_diff = values_current[-len_diff:]\n",
    "                values_current = np.concatenate((values_current, values_diff), axis=0)\n",
    "            new_values = np.append(new_values, values_current)\n",
    "\n",
    "    return new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69437091-ee74-445a-bc0d-cf1be1c65702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_split_indexes(data, start, train_size, test_size, size_unit, n_splits, freq):\n",
    "\n",
    "    '''\n",
    "    Sliding window TS-split\n",
    "    '''\n",
    "    \n",
    "    df = data.copy()\n",
    "\n",
    "    train_window = {size_unit: train_size}\n",
    "    train_offset = pd.offsets.DateOffset(**train_window)\n",
    "\n",
    "    if isinstance(start, str):\n",
    "        start_date = pd.to_datetime(start) - train_offset\n",
    "    else:\n",
    "        start_date = start - train_offset\n",
    "\n",
    "    train_indexes_list = []\n",
    "    test_indexes_list = []\n",
    "\n",
    "    for n in arange(n_splits):\n",
    "        \n",
    "        train_window = {size_unit: train_size}\n",
    "        train_offset = pd.offsets.DateOffset(**train_window)\n",
    "        train_start = start_date\n",
    "        train_end = train_start + train_offset\n",
    "        train_indexes = pd.date_range(train_start, train_end, freq=freq)[:-1]\n",
    "        train_indexes_list.append(train_indexes)\n",
    "        \n",
    "        test_window = {size_unit: test_size}\n",
    "        test_offset = pd.offsets.DateOffset(**test_window)\n",
    "        test_start = start_date + train_offset\n",
    "        test_end = test_start + test_offset\n",
    "        test_indexes = pd.date_range(test_start, test_end, freq=freq)[:-1]\n",
    "        test_indexes_list.append(test_indexes)\n",
    "\n",
    "        start_date = start_date + test_offset\n",
    "\n",
    "    return train_indexes_list, test_indexes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4482a4-a80b-4253-b375-04a976d84c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_model_evaluation(\n",
    "        data, start, train_size, test_size, size_unit, n_splits, freq,\n",
    "        orders, fourier_periods, fourier_orders, fourier_orders_type='iter', exog_variables=None):\n",
    "\n",
    "    time_start = time.time()\n",
    "    \n",
    "    train_indexes, test_indexes = cv_split_indexes(\n",
    "        data, start, train_size, test_size, size_unit, n_splits, freq)\n",
    "\n",
    "    if len(train_indexes) != len(test_indexes):\n",
    "        print('ERROR')\n",
    "\n",
    "    results = {}\n",
    "    results_ = {}\n",
    "    models = {}\n",
    "    models_names = {}\n",
    "    iteration = 0\n",
    "\n",
    "    for split_number in arange(len(train_indexes)):\n",
    "        \n",
    "        train_data = data.loc[train_indexes[split_number]]\n",
    "        test_data = data.loc[test_indexes[split_number]]\n",
    "        \n",
    "        if exog_variables is not None:\n",
    "            exog_variables_train = exog_variables.loc[train_indexes[split_number]]\n",
    "            exog_variables_test = exog_variables.loc[test_indexes[split_number]]\n",
    "\n",
    "        for order in orders:\n",
    "            for fourier_period in fourier_periods:\n",
    "                # if 'iter', than check all combinations of fourier_periods\n",
    "                # e.g. fourier_periods == [1,2,3,4] than check all posibble combinations (1,1,1,1), (1,1,1,2), (1,1,1,3), etc.\n",
    "                if fourier_orders_type == 'iter':\n",
    "                    fourier_orders_for_iteration = itertools.product(fourier_orders, repeat=len(fourier_period))\n",
    "                # if 'mono', than\n",
    "                # e.g. fourier_periods == [1,2,3,4] than check [1,1,1,1], [2,2,2,2], [3,3,3,3], [4,4,4,4]\n",
    "                # for length 'fourier_period' == 4\n",
    "                elif fourier_orders_type == 'mono':\n",
    "                    fourier_orders_for_iteration = [[i]*len(fourier_period) for i in fourier_orders]\n",
    "                else:\n",
    "                    raise ValueError(\"'fourier_orders_type' have to be 'iter' or 'mono'\")\n",
    "                for fourier_order in fourier_orders_for_iteration:\n",
    "                    # create df with fourier exogs for train dataset\n",
    "                    exogs_df_train = pd.DataFrame()\n",
    "                    # create df with fourier exogs for test dataset\n",
    "                    exogs_df_test = pd.DataFrame()\n",
    "                    for p, k in zip(fourier_period, fourier_order):\n",
    "\n",
    "                        fourier_train = statsmodels.tsa.deterministic.Fourier(p, k)\n",
    "                        exogs_train = fourier_train.in_sample(train_data.index)\n",
    "                        exogs_df_train = exogs_train.join(exogs_df_train)\n",
    "\n",
    "                        fourier_test = statsmodels.tsa.deterministic.Fourier(p, k)\n",
    "                        exogs_test = fourier_test.in_sample(test_data.index)\n",
    "                        exogs_df_test = exogs_test.join(exogs_df_test)\n",
    "\n",
    "                    if exog_variables is not None:\n",
    "                        exogs_df_train = exogs_df_train.join(exog_variables_train)\n",
    "                        exogs_df_test = exogs_df_test.join(exog_variables_test)\n",
    "                    \n",
    "                    # fit model\n",
    "                    model = SARIMAX(\n",
    "                        train_data, exog=exogs_df_train,\n",
    "                        order=order,\n",
    "                        seasonal_order=(0, 0, 0, 0),\n",
    "                        initialization='approximate_diffuse',\n",
    "                        # enforce_stationarity=False,\n",
    "                        # enforce_invertibility=False\n",
    "                    ).fit(maxiter=1000, disp=False)\n",
    "\n",
    "                    # calculate steps for forecast\n",
    "                    steps = len(test_data)\n",
    "                    # get forecast\n",
    "                    forecast = model.get_forecast(steps=steps, exog=exogs_df_test)\n",
    "                    # y_pred\n",
    "                    y_pred = forecast.predicted_mean\n",
    "                    # y_test\n",
    "                    y_test = test_data\n",
    "                    # RMSE\n",
    "                    rmse = root_mean_squared_error(y_pred, y_test)\n",
    "\n",
    "                    k_list = list(fourier_order)\n",
    "\n",
    "                    model_name = (tuple(order), list(fourier_period), list(fourier_order))\n",
    "                    model_name_str = f'{tuple(order)}, {list(fourier_period)}, {list(fourier_order)}'\n",
    "\n",
    "                    models_names[model_name_str] = model_name\n",
    "                    \n",
    "                    if model_name_str in results:\n",
    "                        results[model_name_str] = np.append(results[model_name_str], rmse)\n",
    "                    else:\n",
    "                        results[model_name_str] = np.array([rmse])\n",
    "\n",
    "                    print(\n",
    "                    f'Iteration {iteration}' '\\n' \\\n",
    "                    f' - Split: {split_number};' '\\n' \\\n",
    "                    f' - Model: {order};' '\\n' \\\n",
    "                    f' - Fourier period: {fourier_period};' '\\n' \\\n",
    "                    f' - Fourier order {fourier_order}.')\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "    # change models in 'result' by 'modelN'-type names and create separate models list\n",
    "    for (i, key), m in zip(enumerate(results.keys()), models_names.keys()):\n",
    "        # models[f'model{i}'] = key\n",
    "        models[f'model{i}'] = models_names[m]\n",
    "        results_[f'model{i}'] = results[key]\n",
    "\n",
    "    results_full = {}\n",
    "    results_full['models'] = models\n",
    "    results_full['splits'] = results_\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    time_finish = time.time() - time_start\n",
    "    time_finish = dt.timedelta(seconds=np.round(time_finish))\n",
    "\n",
    "    print(f'Execution time: {time_finish}')\n",
    "\n",
    "    return results_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a1ff9-04f3-4e92-9a54-b22b4c50e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_w_fourier(\n",
    "        fourier_period: list,\n",
    "        fourier_order: list,\n",
    "        train_data,\n",
    "        eval_data,\n",
    "        train_exog=None,\n",
    "        eval_exog=None):\n",
    "\n",
    "    # create df with fourier exogs for train dataset\n",
    "    train_df = pd.DataFrame()\n",
    "    # create df with fourier exogs for test dataset\n",
    "    eval_df = pd.DataFrame()\n",
    "    \n",
    "    for p, o in zip(fourier_period, fourier_order):\n",
    "\n",
    "        train_fourier = statsmodels.tsa.deterministic.Fourier(p, o)\n",
    "        train_fourier_exogs = train_fourier.in_sample(train_data.index)\n",
    "        train_df = train_fourier_exogs.join(train_df)\n",
    "\n",
    "        eval_fourier = statsmodels.tsa.deterministic.Fourier(p, o)\n",
    "        eval_fourier_exogs = eval_fourier.in_sample(eval_data.index)\n",
    "        eval_df = eval_fourier_exogs.join(eval_df)\n",
    "\n",
    "    train_df = train_df.join(train_exog)\n",
    "    eval_df = eval_df.join(eval_exog)\n",
    "\n",
    "    return train_df, eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c75f0e-9b17-404e-b7cb-b746dc4ed429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_arima_evaluation_error(\n",
    "        data_train,\n",
    "        data_eval,\n",
    "        order,\n",
    "        exog_train=None,\n",
    "        exog_eval=None,\n",
    "        metric=root_mean_squared_error,\n",
    "        exec_time=True, clear_output_include=True):\n",
    "\n",
    "    '''\n",
    "    Metric: sklearn-like metric with (y_true, y_pred)\n",
    "    '''\n",
    "\n",
    "    time_start = time.time()\n",
    "\n",
    "    model = SARIMAX(\n",
    "        endog=data_train,\n",
    "        exog=exog_train,\n",
    "        order=order,\n",
    "        seasonal_order=(0, 0, 0, 0)\n",
    "    ).fit(maxiter=1000, disp=False)\n",
    "\n",
    "    if clear_output_include:\n",
    "        clear_output()\n",
    "\n",
    "    y_true = data_eval\n",
    "    y_pred = model.get_forecast(steps=len(y_true), exog=exog_eval).predicted_mean\n",
    "\n",
    "    result = metric(y_true, y_pred)\n",
    "    \n",
    "    time_finish = time.time() - time_start\n",
    "    time_finish = dt.timedelta(seconds=np.round(time_finish))\n",
    "\n",
    "    if exec_time:\n",
    "        print(f'Execution time: {time_finish}')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253e652-3abc-4056-bc37-1644e43dc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peaks_indicies(data, boundary):\n",
    "    peaks, _ = scipy.signal.find_peaks(data, height=boundary)\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac4cd9-a4e4-4961-a4e9-6d3a3ed351ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_for_occurrence(text, token, occurrence):\n",
    "    gen = (i for i, l in enumerate(text) if l == token)\n",
    "    for _ in range(occurrence - 1):\n",
    "        next(gen)\n",
    "    return next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32d641-d932-4f7f-9d7c-a29456a1be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        forecast,\n",
    "        slice_series=[None, None],\n",
    "        ci=[80, 95],\n",
    "        palette=None,\n",
    "        alpha_actual=0.75,\n",
    "        alpha_forecast=0.75,\n",
    "        alphas=[0.25, 0.15],\n",
    "        legend=True,\n",
    "        display_intervals=True,\n",
    "        ax=None):\n",
    "\n",
    "    if ax is None: ax = plt.gca()\n",
    "\n",
    "    palette = palette or plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    ci_alphas = [(100 - i)*0.01 for i in ci]\n",
    "    slice_series_ = slice(slice_series[0], slice_series[1])\n",
    "\n",
    "    y_true = y_true[slice_series_]\n",
    "    y_pred = y_pred[slice_series_]\n",
    "    \n",
    "    ax.plot(y_true, color=palette[0], alpha=alpha_actual, label='Actual')\n",
    "    ax.plot(y_pred, color=palette[1], alpha=alpha_forecast, label='Forecast')\n",
    "\n",
    "    if display_intervals:\n",
    "\n",
    "        level0 = forecast.conf_int(alpha=ci_alphas[0])\n",
    "        level1 = forecast.conf_int(alpha=ci_alphas[1])\n",
    "    \n",
    "        label0 = f'Level {ci[0]}%'\n",
    "        label1 = f'Level {ci[1]}%'\n",
    "\n",
    "        level0 = level0[slice_series_]\n",
    "        level1 = level1[slice_series_]\n",
    "\n",
    "        ci_levels = [level0, level1]\n",
    "        ci_labels = [label0, label1]\n",
    "        ci_palette = [palette[2], palette[2]]\n",
    "\n",
    "        for le, la, c, a in zip(ci_levels, ci_labels, ci_palette, alphas):\n",
    "        \n",
    "            ax.fill_between(\n",
    "                x=y_pred.index,\n",
    "                y1=le.iloc[:, 0],\n",
    "                y2=le.iloc[:, 1],\n",
    "                lw=0,\n",
    "                color=c,\n",
    "                alpha=a,\n",
    "                label=la)\n",
    "\n",
    "    if legend:\n",
    "        ax.legend(\n",
    "            **legend_inline(),\n",
    "            **legend_create_handles(\n",
    "                4, kind=['l', 'l', 'r', 'r'],\n",
    "                labels=['Actual', 'Forecast', label0, label1],\n",
    "                colors=[palette[0], palette[1], palette[2], palette[2]],\n",
    "                alphas=[alpha_actual, alpha_forecast, alphas[0], alphas[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cd1c7-f9be-41c1-b423-cd666eb6918d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9a005-25dc-4173-be78-99bf67d0663c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoregression-boosting",
   "language": "python",
   "name": "autoregression-boosting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
